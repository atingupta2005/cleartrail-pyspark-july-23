{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67d4de7a-9232-4361-98f8-2dcbbb26962f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c71e81-3602-4559-8617-19b28fa0c7d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- To calculate results such as the rank, row number e.t.c over a range of input rows\n",
    "- Operate on a group of rows (like frame, partition) and return a single value for every input row\n",
    "- To perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e41ca9-5d27-46aa-b3a0-61bf11bccf07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/data/Predict-Forclosure/Foreclosure-Prediction-Dataset/LMS_31JAN2019.csv</td><td>LMS_31JAN2019.csv</td><td>169623492</td><td>1690383782000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/data/Predict-Forclosure/Foreclosure-Prediction-Dataset/LMS_31JAN2019.csv",
         "LMS_31JAN2019.csv",
         169623492,
         1690383782000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /mnt/data/Predict-Forclosure/Foreclosure-Prediction-Dataset/LMS_31JAN2019.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92476737-08c8-4876-80f5-0a9511f65367",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "home = \"/mnt\"\n",
    "\n",
    "bankData2015 = f\"{home}/data/bank-customers.csv\"\n",
    "\n",
    "retailDataDay = f\"{home}/data/retail-data/by-day/\"\n",
    "retailDataDaySmall = f\"{home}/data/retail-data/by-day/2010-12-02.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c146341f-1f70-4b3c-b77b-d9f5a23034de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5f9fab-ec90-4787-a930-7931c328da92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Specify the Window details\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "# Use the window\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3a1277-c917-46cf-863d-4d8e4c2996b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcd15f5-2a03-49c0-b0db-b70648489e82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd14045d-fd41-4436-b897-68db30d66373",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749efcd0-e5da-4e59-9172-24e8e5954502",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba1bec0-6fa0-4555-a8cc-8955398b7613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## lag Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b2f13-419a-4a09-8683-2a2e1100bb85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|null|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|3000|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8120063-cb2f-45a0-aa50-e8e08a6dbbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## lead Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea427d6-bc01-4542-8421-9d45f5e99b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "444b49b4-68ae-4818-80c3-b0dc2044f1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+------+-----+----+----+\n|employee_name|department|salary|row|   avg|  sum| min| max|\n+-------------+----------+------+---+------+-----+----+----+\n|        Maria|   Finance|  3000|  1|3400.0|10200|3000|3900|\n|        Scott|   Finance|  3300|  2|3400.0|10200|3000|3900|\n|          Jen|   Finance|  3900|  3|3400.0|10200|3000|3900|\n|        Kumar| Marketing|  2000|  1|2500.0| 5000|2000|3000|\n|         Jeff| Marketing|  3000|  2|2500.0| 5000|2000|3000|\n|        James|     Sales|  3000|  1|3760.0|18800|3000|4600|\n|        James|     Sales|  3000|  2|3760.0|18800|3000|4600|\n|       Robert|     Sales|  4100|  3|3760.0|18800|3000|4600|\n|         Saif|     Sales|  4100|  4|3760.0|18800|3000|4600|\n|      Michael|     Sales|  4600|  5|3760.0|18800|3000|4600|\n+-------------+----------+------+---+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d5d2559-2f5e-491e-bfe0-affd935ac428",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f630f52-c861-4dc3-83d5-11da7d76251a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Windows are more flexible than your normal groupBy in selecting your aggregate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a49fd44-1ad7-4d07-ac0f-5da2ef0711f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b132d3-591f-4177-9be8-38d0df7f8cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0536a2b-1117-4348-a71c-c92f3a148e73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shopping_data = \\\n",
    "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
    "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
    "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10),\\\n",
    "('Atin','2018-11-10','Paint',85),('Atin','2018-04-03','Ladder',21),('Atin','2018-06-12','Stool',30),\\\n",
    "('Atin','2018-11-09','Vacuum',45),('Atin','2018-07-13','Bucket',25),('Atin','2018-02-08','Gloves',15),\\\n",
    "('Atin','2018-04-03','Brushes',34),('Atin','2018-09-25','Sandpaper',11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15c10dd-a12d-4a40-8f34-bd498583210a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(shopping_data, ['name','date','product','price']).withColumn('date',F.col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9802fe24-4ab3-41f4-a02f-20565ae29fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- date: date (nullable = true)\n |-- product: string (nullable = true)\n |-- price: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef769c0e-d73b-4f1d-9fdd-6765013e9312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n|name|      date|  product|price|\n+----+----------+---------+-----+\n|Alex|2018-10-10|    Paint|   80|\n|Alex|2018-04-02|   Ladder|   20|\n|Alex|2018-06-22|    Stool|   20|\n|Alex|2018-12-09|   Vacuum|   40|\n|Alex|2018-07-12|   Bucket|    5|\n|Alex|2018-02-18|   Gloves|    5|\n|Alex|2018-03-03|  Brushes|   30|\n|Alex|2018-09-26|Sandpaper|   10|\n|Atin|2018-11-10|    Paint|   85|\n|Atin|2018-04-03|   Ladder|   21|\n|Atin|2018-06-12|    Stool|   30|\n|Atin|2018-11-09|   Vacuum|   45|\n|Atin|2018-07-13|   Bucket|   25|\n|Atin|2018-02-08|   Gloves|   15|\n|Atin|2018-04-03|  Brushes|   34|\n|Atin|2018-09-25|Sandpaper|   11|\n+----+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34548ca8-291e-4c48-9179-c6b85afaf946",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## set the base Window Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f92ec60-0202-4e11-adbd-97455f143f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w0 = Window.partitionBy('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a5c790-6351-4035-83c2-06f23462049e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Moving Average and Running Sum of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd68229-0ff0-4207-bb27-c17865ec3bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n|name|      date|  product|price|avg_to_date|accumulating_sum|max_to_date|max_of_last2|items_to_date|\n+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n|Alex|2018-02-18|   Gloves|    5|        5.0|               5|          5|           5|            1|\n|Alex|2018-03-03|  Brushes|   30|       18.0|              35|         30|          30|            2|\n|Alex|2018-04-02|   Ladder|   20|       18.0|              55|         30|          30|            3|\n|Alex|2018-06-22|    Stool|   20|       19.0|              75|         30|          20|            4|\n|Alex|2018-07-12|   Bucket|    5|       16.0|              80|         30|          20|            5|\n|Alex|2018-09-26|Sandpaper|   10|       15.0|              90|         30|          10|            6|\n|Alex|2018-10-10|    Paint|   80|       24.0|             170|         80|          80|            7|\n|Alex|2018-12-09|   Vacuum|   40|       26.0|             210|         80|          80|            8|\n|Atin|2018-02-08|   Gloves|   15|       15.0|              15|         15|          15|            1|\n|Atin|2018-04-03|   Ladder|   21|       23.0|              70|         34|          21|            3|\n|Atin|2018-04-03|  Brushes|   34|       23.0|              70|         34|          34|            3|\n|Atin|2018-06-12|    Stool|   30|       25.0|             100|         34|          34|            4|\n|Atin|2018-07-13|   Bucket|   25|       25.0|             125|         34|          30|            5|\n|Atin|2018-09-25|Sandpaper|   11|       23.0|             136|         34|          25|            6|\n|Atin|2018-11-09|   Vacuum|   45|       26.0|             181|         45|          45|            7|\n|Atin|2018-11-10|    Paint|   85|       33.0|             266|         85|          85|            8|\n+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('avg_to_date',     F.round(F.avg('price').over(w0.orderBy(F.col('date'))),2))\\\n",
    "  .withColumn('accumulating_sum',F.sum('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_to_date',     F.max('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_of_last2',    F.max('price').over(w0.orderBy(F.col('date')).rowsBetween(-1,Window.currentRow)))\\\n",
    "  .withColumn('items_to_date',   F.count('*').over(w0.orderBy(F.col('date'))))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46b24019-9028-458b-a2f9-39c1ed6db0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Time Lag and Lead for purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767fe9d2-057f-41a5-b4e6-ba781032d3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------------------+-------------------------+\n|name|      date|  product|price|days_from_last_purchase|days_before_next_purchase|\n+----+----------+---------+-----+-----------------------+-------------------------+\n|Alex|2018-02-18|   Gloves|    5|                   null|                       13|\n|Alex|2018-03-03|  Brushes|   30|                     13|                       30|\n|Alex|2018-04-02|   Ladder|   20|                     30|                       81|\n|Alex|2018-06-22|    Stool|   20|                     81|                       20|\n|Alex|2018-07-12|   Bucket|    5|                     20|                       76|\n|Alex|2018-09-26|Sandpaper|   10|                     76|                       14|\n|Alex|2018-10-10|    Paint|   80|                     14|                       60|\n|Alex|2018-12-09|   Vacuum|   40|                     60|                     null|\n|Atin|2018-02-08|   Gloves|   15|                   null|                       54|\n|Atin|2018-04-03|   Ladder|   21|                     54|                        0|\n|Atin|2018-04-03|  Brushes|   34|                      0|                       70|\n|Atin|2018-06-12|    Stool|   30|                     70|                       31|\n|Atin|2018-07-13|   Bucket|   25|                     31|                       74|\n|Atin|2018-09-25|Sandpaper|   11|                     74|                       45|\n|Atin|2018-11-09|   Vacuum|   45|                     45|                        1|\n|Atin|2018-11-10|    Paint|   85|                      1|                     null|\n+----+----------+---------+-----+-----------------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('days_from_last_purchase', F.datediff('date',F.lag('date',1).over(w0.orderBy(F.col('date')))))\\\n",
    "  .withColumn('days_before_next_purchase', F.datediff(F.lead('date',1).over(w0.orderBy(F.col('date'))),'date'))\\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 646759559704821,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Window-Functions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
