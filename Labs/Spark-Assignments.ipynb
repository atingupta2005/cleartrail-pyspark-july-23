{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd1dbb1-6123-4c12-ad48-00cd1d9d1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b11e1c-bd94-4902-ab51-db3bee383a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}]\n",
    "a = sc.parallelize(data1)\n",
    "b = spark.createDataFrame(a)\n",
    "#b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd89e51-7792-43be-83ae-9696ca25d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USeA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USdA'},{'Name':'Joe','ID':5.33,'Add':'rsa'}]\n",
    "c = sc.parallelize(data2)\n",
    "d = spark.createDataFrame(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d551e6-ad60-4d56-8d6f-f321a0626341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (Add#6 = Add#0)\n",
      ":- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Add: string, ID: double, Name: string, Add: string, ID: double, Name: string\n",
      "Join Inner, (Add#6 = Add#0)\n",
      ":- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (Add#6 = Add#0)\n",
      ":- Filter isnotnull(Add#6)\n",
      ":  +- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- Filter isnotnull(Add#0)\n",
      "   +- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [Add#6], [Add#0], Inner\n",
      "   :- Sort [Add#6 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(Add#6, 200), ENSURE_REQUIREMENTS, [id=#136]\n",
      "   :     +- Filter isnotnull(Add#6)\n",
      "   :        +- Scan ExistingRDD[Add#6,ID#7,Name#8]\n",
      "   +- Sort [Add#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(Add#0, 200), ENSURE_REQUIREMENTS, [id=#137]\n",
      "         +- Filter isnotnull(Add#0)\n",
      "            +- Scan ExistingRDD[Add#0,ID#1,Name#2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = d.join(b,d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb40fe-d7d7-4188-baa1-7eb4596fd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b3e8ed2-5c30-4d38-ba69-dd3c0d866d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (Add#6 = Add#0)\n",
      ":- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Add: string, ID: double, Name: string, Add: string, ID: double, Name: string\n",
      "Join Inner, (Add#6 = Add#0)\n",
      ":- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (Add#6 = Add#0), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(Add#6)\n",
      ":  +- LogicalRDD [Add#6, ID#7, Name#8], false\n",
      "+- Filter isnotnull(Add#0)\n",
      "   +- LogicalRDD [Add#0, ID#1, Name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Add#6], [Add#0], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Add#6)\n",
      "   :  +- Scan ExistingRDD[Add#6,ID#7,Name#8]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#161]\n",
      "      +- Filter isnotnull(Add#0)\n",
      "         +- Scan ExistingRDD[Add#0,ID#1,Name#2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "f = d.join(broadcast(b),d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef83d58c-fdb2-4725-adf2-09725d601bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---+------+----+\n",
      "|Add|    ID|Name|Add|    ID|Name|\n",
      "+---+------+----+---+------+----+\n",
      "|USA|21.528|Jhon|USA| 22.22|Jhon|\n",
      "|USA|21.528|Jhon|USA|  3.69| Joe|\n",
      "|USA|21.528|Jhon|USA|21.528|Jhon|\n",
      "|IND|  2.48|Tina|IND|  2.48|Tina|\n",
      "+---+------+----+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31878df1-265f-40ef-a068-8382835bb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|     city|\n",
      "+----------+---------+\n",
      "|    andrea| medellin|\n",
      "|   rodolfo| medellin|\n",
      "|     abdul|bangalore|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF = (\n",
    "  (\"andrea\", \"medellin\"),\n",
    "  (\"rodolfo\", \"medellin\"),\n",
    "  (\"abdul\", \"bangalore\")\n",
    ")\n",
    "\n",
    "peopleDF_a = sc.parallelize(peopleDF)\n",
    "peopleDF_b = spark.createDataFrame(peopleDF_a, [\"first_name\", \"city\"])\n",
    "\n",
    "peopleDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3654c5-fceb-4faa-bf10-8e4d9c7377fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+\n",
      "|     city| country|population|\n",
      "+---------+--------+----------+\n",
      "| medellin|colombia|       2.5|\n",
      "|bangalore|   india|      12.3|\n",
      "+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citiesDF = (\n",
    "  (\"medellin\", \"colombia\", 2.5),\n",
    "  (\"bangalore\", \"india\", 12.3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "citiesDF_a = sc.parallelize(citiesDF)\n",
    "citiesDF_b = spark.createDataFrame(citiesDF_a, (\"city\", \"country\", \"population\"))\n",
    "\n",
    "citiesDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a887da3-eb98-4d17-8234-b5c9de17755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#112 = city#124)\n",
      ":- LogicalRDD [first_name#111, city#112], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#112 = city#124)\n",
      ":- LogicalRDD [first_name#111, city#112], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#112 = city#124), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(city#112)\n",
      ":  +- LogicalRDD [first_name#111, city#112], false\n",
      "+- Filter isnotnull(city#124)\n",
      "   +- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [city#112], [city#124], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(city#112)\n",
      "   :  +- Scan ExistingRDD[first_name#111,city#112]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#515]\n",
      "      +- Filter isnotnull(city#124)\n",
      "         +- Scan ExistingRDD[city#124,country#125,population#126]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  broadcast(citiesDF_b),\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22abbdf6-6c88-40cb-9597-e080a9e1f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#112 = city#124)\n",
      ":- LogicalRDD [first_name#111, city#112], false\n",
      "+- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#112 = city#124)\n",
      ":- LogicalRDD [first_name#111, city#112], false\n",
      "+- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#112 = city#124)\n",
      ":- Filter isnotnull(city#112)\n",
      ":  +- LogicalRDD [first_name#111, city#112], false\n",
      "+- Filter isnotnull(city#124)\n",
      "   +- LogicalRDD [city#124, country#125, population#126], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [city#112], [city#124], Inner\n",
      "   :- Sort [city#112 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(city#112, 200), ENSURE_REQUIREMENTS, [id=#564]\n",
      "   :     +- Filter isnotnull(city#112)\n",
      "   :        +- Scan ExistingRDD[first_name#111,city#112]\n",
      "   +- Sort [city#124 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(city#124, 200), ENSURE_REQUIREMENTS, [id=#565]\n",
      "         +- Filter isnotnull(city#124)\n",
      "            +- Scan ExistingRDD[city#124,country#125,population#126]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  citiesDF_b,\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab568f5-5dd8-4ca6-b4b4-7c5103892a04",
   "metadata": {},
   "source": [
    "#### Notice how the parsed, analyzed, and optimized logical plans all contain ResolvedHint isBroadcastable=true because the broadcast() function was used. This hint isn’t included when the broadcast() function isn’t used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dcb2b7-bec5-4269-a431-83dfa5efbdbe",
   "metadata": {},
   "source": [
    "## Automatic DetectionPermalink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64273b2-7737-43bf-9986-b9b769ad6620",
   "metadata": {},
   "source": [
    "- In many cases, Spark can automatically detect whether to use a broadcast join or not, depending on the size of the data. If Spark can detect that one of the joined DataFrames is small (10 MB by default), Spark will automatically broadcast it for us. The code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3e63623-f747-493a-8bc7-1dea79008e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(id))\n",
      ":- Range (1, 10000, step=1, splits=Some(4))\n",
      "+- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [id#276L]\n",
      "+- Join Inner, (id#276L = id#274L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#276L]\n",
      "+- Join Inner, (id#276L = id#274L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#276L]\n",
      "   +- BroadcastHashJoin [id#276L], [id#274L], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#585]\n",
      "      :  +- Range (1, 10000, step=1, splits=4)\n",
      "      +- Range (1, 100000000, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigTable = spark.range(1, 100000000)\n",
    "smallTable = spark.range(1, 10000) # size estimated by Spark - auto-broadcast\n",
    "joinedNumbers = smallTable.join(bigTable, \"id\")\n",
    "joinedNumbers.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5acb7-18d6-4894-b83d-0b0fdf12511a",
   "metadata": {},
   "source": [
    "- However, in the previous case, Spark did not detect that the small table could be broadcast. \n",
    "- The reason is that Spark will not determine the size of a local collection because it might be big, and evaluating its size may be an O(N) operation, which can defeat the purpose before any computation is made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc7377-f588-45a1-ba28-e5661ecc1a43",
   "metadata": {},
   "source": [
    "- Spark will perform auto-detection when\n",
    "    - it constructs a DataFrame from scratch, e.g. spark.range\n",
    "    - it reads from files with schema and/or size information, e.g. Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf2034-2e12-4aa4-b3cd-49bb9d1fef13",
   "metadata": {},
   "source": [
    "## Configuring Broadcast Join Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5033ff2-bd71-4140-ae3e-7699ac890a51",
   "metadata": {},
   "source": [
    "- The threshold for automatic broadcast join detection can be tuned or disabled.\n",
    "- The configuration is spark.sql.autoBroadcastJoinThreshold, and the value is taken in bytes. If you want to configure it to another number, we can set it in the SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d0b3690-ba01-48ad-999b-56f094c2d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c9695-0880-442b-8070-2a1e995a771a",
   "metadata": {},
   "source": [
    "- or deactivate it altogether by setting the value to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51620fb2-8ff7-42fb-99f4-e6d0a17a2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a649559-1a41-4993-b2e7-416435ccb136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18d3ea48-3fc3-43af-9819-44355a3ff861",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c6a6a-187c-4a5b-8fed-bcf5a4e0351f",
   "metadata": {},
   "source": [
    "- Import the broadcast() method from pyspark.sql.functions.\n",
    "- Create a new DataFrame broadcast_df by joining flights_df with airports_df, using the broadcasting.\n",
    "- Show the query plan and consider differences from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b0f5c-810b-42e9-852c-7827c3a793b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from ____ import ____\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.____(____(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728612b-f910-423e-8fed-bcd2c303c177",
   "metadata": {},
   "source": [
    "- Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "    - Name\n",
    "    - Age\n",
    "    - City\n",
    "\n",
    "- The Name and City columns are StringType() and the Age column is an IntegerType()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b237899-010c-43c9-9056-2719471a112b",
   "metadata": {},
   "source": [
    "- Import * from the pyspark.sql.types library.\n",
    "- Define a new schema using the StructType method.\n",
    "- Define a StructField for name, age, and city. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee111f8-101c-4a76-a62b-c74f3a75275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "____\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = ____([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', ____, False),\n",
    "  ____('____', IntegerType(), ____)\n",
    "  ____\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bb932-f9cc-4da4-9ba8-5a1d84a8e7df",
   "metadata": {},
   "source": [
    "- Load the Data Frame.\n",
    "- Add the transformation for F.lower() to the Destination Airport column.\n",
    "- Drop the Destination Airport column from the Data Frame aa_dfw_df. Note the time for these operations to complete.\n",
    "- Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9655d2-a718-4b3c-a53c-207e7518dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = ____.____.____('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', ____(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = ____(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31ac28-06c8-478a-846e-615644599b39",
   "metadata": {},
   "source": [
    "- View the row count of df1 and df2.\n",
    "- Combine df1 and df2 in a new DataFrame named df3 with the union method.\n",
    "- Save df3 to a parquet file named AA_DFW_ALL.parquet.\n",
    "- Read the AA_DFW_ALL.parquet file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2732d60-ac19-4fb6-8dd8-67eced359541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.____())\n",
    "print(\"df2 Count: %d\" % ____.____())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.____.____('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.____('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a801e75-ca07-41d0-adfe-7a00cd65ebc9",
   "metadata": {},
   "source": [
    "- Import the AA_DFW_ALL.parquet file into flights_df.\n",
    "- Use the createOrReplaceTempView method to alias the flights table.\n",
    "- Run a Spark SQL query against the flights table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cbf80-3a05-4032-a26c-cc9264a31ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.____(____)\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.____('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.____('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce9d93-af94-4d6c-bc20-b616fbab8810",
   "metadata": {},
   "source": [
    "## Comparing broadcast vs normal joins\n",
    "- You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "- Your DataFrames normal_df and broadcast_df are available for your use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f05f5-ab6c-407b-bd5c-2c15f65d91fd",
   "metadata": {},
   "source": [
    "- Execute .count() on the normal DataFrame.\n",
    "- Execute .count() on the broadcasted DataFrame.\n",
    "- Print the count and duration of the DataFrames noting and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa00d8-ea5b-4563-893c-5ab0e4d932f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = ____\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = ____\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33802a79-c868-4825-8aac-04ceed6500fd",
   "metadata": {},
   "source": [
    "- Import the file 2015-departures.csv.gz to a DataFrame. Note the header is already defined.\n",
    "- Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use .printSchema() to see the column names / order).\n",
    "- Add an ID column.\n",
    "- Write the file out as a JSON document named output.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a0072-15c1-45fa-9ca1-08133eaf7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.____(____, header=____)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.____(____)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.____('id', ____)\n",
    "\n",
    "# Write the file out to JSON format\n",
    "____.write.____(____, mode='overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
