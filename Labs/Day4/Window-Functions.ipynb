{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73051a54-c249-47f9-88a5-0b462b031e53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64ce226e-192c-4548-91ed-6562ba3d1171",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- To calculate results such as the rank, row number e.t.c over a range of input rows\n",
    "- Operate on a group of rows (like frame, partition) and return a single value for every input row\n",
    "- To perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297ca579-1af4-49e0-a7a7-ccb720ac0ed0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500e152c-fcf3-4173-adab-103e5a01f714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf420afa-c991-46cd-a536-3449cddf0b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Specify the Window details\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "# Use the window\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29dc73f8-772c-4c77-92ca-56e4640cd9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6069831-5038-425f-84e9-6822093475a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "668e1e1d-fc4b-4215-9400-10045e1ef8c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b0f92d-2aa7-4251-aac5-2de82ca4eb8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d5eea9-e37b-4a74-bf3d-2ea95f9f2b25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## lag Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee6216b-7dca-465e-884d-1043212c59c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28fe9d7e-e19d-4f7e-acc4-c99f4518c6c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## lead Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c461bc0-7866-4d41-b419-2281eee1a45b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3893e0b5-e00e-4e20-837e-7e3c037cff1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+------+-----+----+----+\n|employee_name|department|salary|row|   avg|  sum| min| max|\n+-------------+----------+------+---+------+-----+----+----+\n|        James|     Sales|  3000|  1|3760.0|18800|3000|4600|\n|        James|     Sales|  3000|  2|3760.0|18800|3000|4600|\n|       Robert|     Sales|  4100|  3|3760.0|18800|3000|4600|\n|         Saif|     Sales|  4100|  4|3760.0|18800|3000|4600|\n|      Michael|     Sales|  4600|  5|3760.0|18800|3000|4600|\n|        Maria|   Finance|  3000|  1|3400.0|10200|3000|3900|\n|        Scott|   Finance|  3300|  2|3400.0|10200|3000|3900|\n|          Jen|   Finance|  3900|  3|3400.0|10200|3000|3900|\n|        Kumar| Marketing|  2000|  1|2500.0| 5000|2000|3000|\n|         Jeff| Marketing|  3000|  2|2500.0| 5000|2000|3000|\n+-------------+----------+------+---+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb58212-0283-4032-b2a8-10809f77da94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b6de53-9330-4926-8ab9-95f4367380ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Windows are more flexible than your normal groupBy in selecting your aggregate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebf8f0b8-a99d-4e0f-be52-7c8ebc3b57a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07469c5b-4c61-4987-af8f-0396b26888f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext,SparkConf,SparkContext,HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b75b55c-6db2-498a-93a1-7d4e7d480c83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shopping_data = \\\n",
    "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
    "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
    "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25791b4-a1ef-4935-9871-bceb6a5f1b1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(shopping_data, ['name','date','product','price']).withColumn('date',F.col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c1ba2d-21d6-4a6b-af9d-2d250f74bb28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- date: date (nullable = true)\n |-- product: string (nullable = true)\n |-- price: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bdf606-b169-4650-b315-a5aaf1ff9ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n|name|      date|  product|price|\n+----+----------+---------+-----+\n|Alex|2018-10-10|    Paint|   80|\n|Alex|2018-04-02|   Ladder|   20|\n|Alex|2018-06-22|    Stool|   20|\n|Alex|2018-12-09|   Vacuum|   40|\n|Alex|2018-07-12|   Bucket|    5|\n|Alex|2018-02-18|   Gloves|    5|\n|Alex|2018-03-03|  Brushes|   30|\n|Alex|2018-09-26|Sandpaper|   10|\n+----+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec724b7-742e-4212-9f90-451efd190b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## set the base Window Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9f58d59-b178-4666-bb5c-46c08746f7f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w0 = Window.partitionBy('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4288fa6b-af97-4840-b3bc-8587a8785adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Moving Average and Running Sum of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77d48154-649a-48cc-abfb-309c42c924c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n|name|      date|  product|price|avg_to_date|accumulating_sum|max_to_date|max_of_last2|items_to_date|\n+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n|Alex|2018-02-18|   Gloves|    5|        5.0|               5|          5|           5|            1|\n|Alex|2018-03-03|  Brushes|   30|       17.5|              35|         30|          30|            2|\n|Alex|2018-04-02|   Ladder|   20|      18.33|              55|         30|          30|            3|\n|Alex|2018-06-22|    Stool|   20|      18.75|              75|         30|          20|            4|\n|Alex|2018-07-12|   Bucket|    5|       16.0|              80|         30|          20|            5|\n|Alex|2018-09-26|Sandpaper|   10|       15.0|              90|         30|          10|            6|\n|Alex|2018-10-10|    Paint|   80|      24.29|             170|         80|          80|            7|\n|Alex|2018-12-09|   Vacuum|   40|      26.25|             210|         80|          80|            8|\n+----+----------+---------+-----+-----------+----------------+-----------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('avg_to_date',     F.round(F.avg('price').over(w0.orderBy(F.col('date'))),2))\\\n",
    "  .withColumn('accumulating_sum',F.sum('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_to_date',     F.max('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_of_last2',    F.max('price').over(w0.orderBy(F.col('date')).rowsBetween(-1,Window.currentRow)))\\\n",
    "  .withColumn('items_to_date',   F.count('*').over(w0.orderBy(F.col('date'))))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37512621-8e07-4c24-a95a-bb96ce88e5af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Time Lag and Lead for purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48ea84d-e56a-4cf3-8c35-31359f47fca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------------------+-------------------------+\n|name|      date|  product|price|days_from_last_purchase|days_before_next_purchase|\n+----+----------+---------+-----+-----------------------+-------------------------+\n|Alex|2018-02-18|   Gloves|    5|                   null|                       13|\n|Alex|2018-03-03|  Brushes|   30|                     13|                       30|\n|Alex|2018-04-02|   Ladder|   20|                     30|                       81|\n|Alex|2018-06-22|    Stool|   20|                     81|                       20|\n|Alex|2018-07-12|   Bucket|    5|                     20|                       76|\n|Alex|2018-09-26|Sandpaper|   10|                     76|                       14|\n|Alex|2018-10-10|    Paint|   80|                     14|                       60|\n|Alex|2018-12-09|   Vacuum|   40|                     60|                     null|\n+----+----------+---------+-----+-----------------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('days_from_last_purchase', F.datediff('date',F.lag('date',1).over(w0.orderBy(F.col('date')))))\\\n",
    "  .withColumn('days_before_next_purchase', F.datediff(F.lead('date',1).over(w0.orderBy(F.col('date'))),'date'))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1fb2bed-9f1c-452b-8015-37bb603d2f84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List and Sets of purchases by member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44bf7dc-29bf-4064-9778-81156422ab1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------------+-----------------------+\n|name|date      |product  |price|items_by_price  |all_prices             |\n+----+----------+---------+-----+----------------+-----------------------+\n|Alex|2018-07-12|Bucket   |5    |[Bucket, Gloves]|[30, 5, 20, 10, 40, 80]|\n|Alex|2018-02-18|Gloves   |5    |[Bucket, Gloves]|[30, 5, 20, 10, 40, 80]|\n|Alex|2018-09-26|Sandpaper|10   |[Sandpaper]     |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-10-10|Paint    |80   |[Paint, Paint]  |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-10-11|Paint    |80   |[Paint, Paint]  |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-03-03|Brushes  |30   |[Brushes]       |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-04-02|Ladder   |20   |[Ladder, Stool] |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-06-22|Stool    |20   |[Ladder, Stool] |[30, 5, 20, 10, 40, 80]|\n|Alex|2018-12-09|Vacuum   |40   |[Vacuum]        |[30, 5, 20, 10, 40, 80]|\n+----+----------+---------+-----+----------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "newRow = spark.createDataFrame([('Alex','2018-10-11','Paint',80)])\n",
    "df2 = df.union(newRow)\n",
    "\n",
    "df2.withColumn('items_by_price', F.collect_list('product').over(w0.partitionBy('price')))\\\n",
    "   .withColumn('all_prices',     F.collect_set('price').over(w0)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4334e06b-ee6b-405c-a534-3e5c925489bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+\n|name|Price|           items|\n+----+-----+----------------+\n|Alex|    5|[Bucket, Gloves]|\n|Alex|   10|     [Sandpaper]|\n|Alex|   80|         [Paint]|\n|Alex|   30|       [Brushes]|\n|Alex|   20| [Ladder, Stool]|\n|Alex|   40|        [Vacuum]|\n+----+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('items', F.collect_set('product').over(w0.partitionBy('price')))\\\n",
    "   .select('name','Price','items')\\\n",
    "   .distinct()\\\n",
    "   .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11-Window-Functions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
